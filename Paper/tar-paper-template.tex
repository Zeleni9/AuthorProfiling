% Paper template for TAR 2016
% (C) 2014 Jan Šnajder, Goran Glavaš, Domagoj Alagić, Mladen Karan
% TakeLab, FER

\documentclass[10pt, a4paper]{article}

\usepackage{tar2016}

\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{scrextend}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{enumitem}

\title{ Author profiling}

\name{Filip Zelić, Borna Sirovica, Ivan-Dominik Ljubičić} 

\address{
University of Zagreb, Faculty of Electrical Engineering and Computing\\
Unska 3, 10000 Zagreb, Croatia\\ 
\texttt{filip.zelic@fer.hr,
borna.sirovica@fer.hr,
ivan-dominik.ljubicic@fer.hr}\\
}
          
         
\abstract{ 
\newgeometry{left=-5.2cm,right=-1cm}
In this paper, we present our approach to the author profiling task, a student project for Text Analysis and Retrieval course. Given a set of tweets by the same person, the task aims at identifying age, gender and personality traits of that person. We address age and gender prediction as a classification task and a personality prediction as a regression problem. We experimented with Support Vector Machine for classification and regression and other machine learning algorithms using a variety of custom designed features as well as features extracted from publicly available resources.
}

\begin{document}

\maketitleabstract

\section{Introduction}

Author profiling distinguishes between classes of authors by studying their sociolect aspect, i.e., how language is shared or how an author can be characterized from a psychological viewpoint. This information helps in identifying profiling aspects such as gender, age, native language, or personality type. Author profiling is a problem of growing importance, among others for applications in forensics, security, and marketing. However, social profiling still remains a less-explored topic, even though the exponential growth of social networks increased its importance even further. While there were several publications that were trying to predict some demographical information such as gender, age, and native language \citep{argamon2003}, \citep{peersman2011}, as well as the personality type, and to perform author profiling in general \citep{argamon2009}, the real push forward was enabled by specialized competitions such as the PAN shared tasks on author profiling \citep{pardo2013}, \citep{rangel2014}, \citep{rangel2015}, which ran in 2013–2015. 

This paper presents our approach for the author profiling task. The task focused on predicting an author’s demographics (age and gender) and the big five personality traits \citep{mccrae2008} (agreeable, conscientious, extroverted, open, stable) from the text of a set of tweets by the same target author. Corpora contained tweets from 152 users in the English language.

% Promijeniti 
 We experimented with Support Vector Machine Classification, Regression and other machine learning algorithms using a variety of custom designed features as well as features extracted from publicly available resources. 


This paper is organized as follows: Section 2 presents general approach and methodology used in the experiments, including a description of the preprocessing, the features, and the learning algorithms we used. Section 3 presents and discusses our results and provides some deeper analysis. Finally, Section 5 concludes and points to possible directions for future work.

\section{Dataset}

The dataset we used consisted of English tweets from 152 users in \textit{.xml} format along with one \textit{truth.txt} file with age, gender and the Big Five personality traits labels for each user \citep{dataset2015} . For labeling age, the following classes were considered: 1) 18-24, 2) 25-34, 3) 35-49, 4) 50+ and gender was labeled as male (M) or female (F). The distribution of the gender and age labels in the corpus is reported in Table~\ref{tab:narrow-table-1}. For the case of gender classes the corpus was balanced with 50\% of the tweets labeled as male and other half as female, but regarding age the distribution was skewed due to the lower number (around 22\%) of the older users (labels \textit{35-49} and \textit{50+}) and higher number (around 78\%) of the younger users (labels \textit{18-24} and \textit{25-34}).
\begin{table}[h]
\caption{Distribution of Twitter users with respect to the age and gender labels in the corpus.}
\label{tab:narrow-table-1}
\vspace{-3mm}
\begin{center}
\begin{tabular}{llc}
\toprule
Trait & Label & Number of users\\
\midrule
\multirow{4}{*}{Age}
		&18-24   & 58\\
    &25-34   & 60\\
    &35-49   & 22\\
    &50+   & 12\\
\midrule
\multirow{2}{*}{Gender}
		&Male   & 76\\
    &Female & 76\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Regarding personality traits normalized numeric rating in [-0.5,0.5] range was given for each of the following properties:  extroverted, stable, agreeable, conscientious, and open. The mean for each trait is reported in Table~\ref{tab:narrow-table-2}.
\begin{table}[h]
\caption{Mean values of the Big Five personality traits in the corpus.}
\label{tab:narrow-table-2}
\vspace{-3mm}
\begin{center}
\begin{tabular}{lc}
\toprule
Personality trait & Mean value\\
\midrule
Extroverted  & 0.16\\
Stable   & 0.14\\
Agreeable  & 0.12\\
Conscientious     & 0.17\\
Open     & 0.24\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\section{Approach}

Along this section, we describe the steps we took to prepare features for the training of our supervised machine learning models. In subsection 3.1 we explain the preprocessing step in which we cleaned the data in order to get better performances for stylometric features. Subsection 3.2 explains extraction of various features for each separate subtask (age, gender and personality traits) and last subsection 3.3 describes classification and regression models we used tackling the task of author profiling.

\subsection{Preprocessing}

Preprocessing is done by creating a document for each user by joining all his/hers tweets from the dataset. After the creation, document is initally striped of XML tags and cleared of all twitter specific characteristics such as hashtags, @replies as well as URLs from the text. While doing this step we save the count of mentions, hashtags and URLs for later use. When the preprocessing step is done, features need to be extracted. 

\subsection{Feature extraction} 

As mentioned in the introduction, we used many different self-designed features and features extracted from publicly available resources for our machine learning algorithms.
 
\textbf{Lexicon Features}. We used several lexicons, both manually crafted and automatically generated:
\begin{itemize}[noitemsep,nolistsep]
\item \textbf{NRC Word-Emotion Association Lexicon} []: one dictionary for each of the eight primary human emotions [//ako postoji neki paper]: anger, anticipation, disgust, fear, joy, sadness, surprise, trust as well as dictionaries for positive and negative emotion word classification
\item \textbf{Internet slang word lexicon} : personally made dictionary with most common words younger people like to use on the Internet
\item \textbf{Frequent male and female words lexicon} [] : dictionaries containing words most likely to be used by male or female person. Based on word usage frequency per million words.
\item \textbf{Frequent male and female words lexicon} [??] : dictionaries containing function words, such as 'this' or 'not', most likely to be used by male or female person.
\item \textbf{World Well-Being Project Personality Lexicon } [bugari i link na članak]:  top and bottom 100 words for each of the five traits, a total of 1000 words.
\end{itemize}
For each lexicon, we found the number of occurrences of lexicon terms in all tweets for each user, normalized by the total number of words in tweets. We instantiated separate features for the different lexicons.

\textbf{Twitter-specific Features.} We used the following Twitter-specific features:
\begin{itemize}[noitemsep,nolistsep]
\item \textbf{Letter case:} the number of upper-case words and upper-case characters
\item \textbf{Hashtags:} the number of hashtags
\item \textbf{URLs:} the number of URLs posted
\item \textbf{User mentions:} the number of mentions of users using the pattern @username
\item \textbf{Emoticons:} the number of emoticons used in tweets
\end{itemize}
All of the above counts are normalized by the total number of available tweets
for the target user; so they could be viewed as “average number per tweet”.

\textbf{Orthographic Features.} We used the following orthographic features:
\begin{itemize}[noitemsep,nolistsep] 
\item \textbf{Character flooding:} the number of redundant character reduplication 
\item \textbf{Word length:} average word length
\item \textbf{Tweet length:} average tweet length
\item \textbf{Specific characters:} usage of specific characters, such as the number of occurrences of the exclamation points, question marks or apostrophe in tweets
\end{itemize}

\textbf{Term-level Features.} We used the following term-level features:
\begin{itemize}[noitemsep,nolistsep] 
\item \textbf{n-grams:}: presence and count of unigrams and trigrams //borna
\item \textbf{POS tagging:} //borna
\end{itemize}

\subsection{Supervised learning models}

Supervised machine learning models are used with hiperparameter optimization using grid search and k-fold cross validation on  70\% of training data. Regarding classification task for age and gender, we tested the following classification models: 
\begin{itemize}
	\item[-] Support Vector Machine Classification (SVC) with linear kernel and rbf kernel
	\item[-] Logistic Regression
	\item[-] Random Forest
\end{itemize}

On the other hand regarding regression task for personality traits, we tested following regression models:  
\begin{itemize}
	\item[-] Support Vector Machine Regression (SVR) with linear kernel and rbf kernel
	\item[-] Linear Regression
	\item[-] Polynomial Regression	
\end{itemize}

Features are scaled using StandardScaler from sklearn python library in order to avoid complications that can arise in the classification stage due to features with numeric values that differ a lot.


\section{Evaluation}



\section{Conclusion and future work}

Conclusion is the last enumerated section of the paper. It should not exceed half of a column and is typically split into 2--3 paragraphs. No new information should be presented in the conclusion; this section only summarizes and concludes the paper.


\bibliographystyle{tar2016}
\bibliography{tar2016} 

\end{document}

